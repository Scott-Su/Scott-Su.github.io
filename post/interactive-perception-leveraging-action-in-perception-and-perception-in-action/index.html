<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>Interactive Perception: Leveraging Action in Perception and Perception in Action | ScottSu的个人学习小站</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://scott-su.github.io//favicon.ico?v=1581935875854">
<link rel="stylesheet" href="https://scott-su.github.io//styles/main.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="本篇综述在对交互感知(Interactive Perception,IP)的一些基础概念与历史发展的介绍与厘清后，给出了一个比较清晰的标准来区分IP是以怎样的方式被应用到各类领域。
基本概念
Forceful Interactions：

..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://scott-su.github.io/">
        <img src="https://scott-su.github.io//images/avatar.png?v=1581935875854" class="site-logo">
        <h1 class="site-title">ScottSu的个人学习小站</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      保持耐心
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://scott-su.github.io//atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">Interactive Perception: Leveraging Action in Perception and Perception in Action</h2>
            <div class="post-date">2020-02-17</div>
            
            <div class="post-content" v-pre>
              <p>本篇综述在对交互感知(Interactive Perception,IP)的一些基础概念与历史发展的介绍与厘清后，给出了一个比较清晰的标准来区分IP是以怎样的方式被应用到各类领域。</p>
<h1 id="基本概念">基本概念</h1>
<p>Forceful Interactions：</p>
<blockquote>
<p>Any <strong>action</strong> that exerts a potentially time-varying force upon the environment is a forceful interaction</p>
</blockquote>
<p>Create Novel Signals (CNS): 通过交互产生所返回的各类传感器信号。如触觉、视觉数据。</p>
<p>Action Perception Regularity (APR)：<br>
这些数据或者信号所具有的内部结构与特征（Regularity，个人理解，类似于CNN从一堆图像中提取出高层次的共同的特征与架构即feature）。</p>
<blockquote>
<p>Forceful interactions reveal regularities in the combined space <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>×</mo><mi>A</mi><mo>×</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">S×A×t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span> of sensor information (S) and action parameters (A) over time (t).</p>
<p>This regularity is constituted by the repeatable, multi-modal sensory data that is created when executing the same action in the same environment.</p>
<p>Knowing this regularity corresponds to understanding the causal <strong>relationship between action and sensory response</strong> given specific environment properties.</p>
</blockquote>
<p>下图是对各种Perception方法的一个简单的总结与介绍。实际上Active Perception跟Interactive Perception 的关系也变得模糊了，之所以这么分，是因为在这篇综述之前的大部分AP方法还是主要单单基于视觉进行观察。所以不满足<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>的条件</p>
<figure data-type="image" tabindex="1"><img src="https://scott-su.github.io//post-images/1581935639450.png" alt="" loading="lazy"></figure>
<h2 id="ip的应用与分类">IP的应用与分类</h2>
<h3 id="应用">应用</h3>
<figure data-type="image" tabindex="2"><img src="https://scott-su.github.io//post-images/1581935659153.png" alt="" loading="lazy"></figure>
<p>这里以关节模型估计（Articulation Model Estimation）和基于触觉的属性估计（Haptic Property Estimation）为例进行简单说明：<br>
<img src="https://scott-su.github.io//post-images/1581935663208.png" alt="" loading="lazy"></p>
<p>关节模型估计：如上图所示，机器人（由立体摄像机和视锥指示）试图估计桌上两个乐高积木的关节模型。在不同的情况下，机器人可以获得的信息量是不同的。[左]机器人只能改变视角以获取更多信息。[中]机器人可以观察到一个人举起乐高积木时产生的丰富的感觉信号。[右]机器人可以与场景交互并观察产生的感官信号。因此，它通过指定的交互可以得到更多信息。只有在最右边的情况下，才能可靠地评估出关节模型。<br>
<img src="https://scott-su.github.io//post-images/1581935669825.png" alt="" loading="lazy"></p>
<p>基于触觉的属性估计：如上图所示，机器人试图估计球体的重量。在不同的情况下，机器人可以获得的信息量是不同的。[左]机器人只能改变视角以获取更多信息。[中]机器人能观察到一个人推球产生的丰富的感觉信号。[右]机器人可以推动球体本身，观察产生的感官信号，即球体静止的位置。在最后一种情况下，它通过指定的推力可以得到更多信息。只有在最右边的情况下，才能够可靠的评估出球体的质量。</p>
<h3 id="分类">分类</h3>
<figure data-type="image" tabindex="3"><img src="https://scott-su.github.io//post-images/1581935870348.png" alt="" loading="lazy"></figure>
<p><br>
<img src="https://scott-su.github.io//post-images/1581935874281.png" alt="" loading="lazy"></p>
<p>这里的分类基于5大标准(A-F):</p>
<p>A. How is the signal in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>×</mo><mi>A</mi><mo>×</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">S×A×t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span> leveraged?</p>
<p>只要是使用到APR的IP方法，肯定都会有用到CNS的。前面提到过，其实这里的APR更具体地表述为先验知识（prior knowledge）,也是一种限制，更是一种通过交互来对内部特征的利用。</p>
<blockquote>
<p>A prior is a source of information that aids in the interpretation of the sensor signal by rejecting noise, possibly by projecting the signal into a lower dimensional space.<br>
<img src="https://scott-su.github.io//post-images/1581935787456.png" alt="" loading="lazy"><br>
比如要求物体是刚体（RO），机器人给定有push,grasp等操作，实际上就是对APR（所谓的regularity）的利用程度。上图所示最左边单纯靠视觉去追踪，其他啥也不用做，也不需要物体限制或者机器人操作；最右边就得满足很多限制要求的同时还会通过机械爪深度的进行交互，充分利用各类型传感器返回的各种信息。这就是exploit的差距。</p>
</blockquote>
<p>B. What priors are employed?<br>
具体分为基于动力学模型的prior（<em>Priors on the Dynamics</em>），还有基于观察的prior(<em>Priors on the Observations</em>)。</p>
<h4 id="priors-on-the-dynamics">Priors on the Dynamics</h4>
<p>a)Given/Specified/Engineered Priors</p>
<p>很好理解,如前所述,例如要求物体是刚体(rigid),机械臂在平面上(plane)进行push操作。</p>
<p>b)Learned Priors</p>
<p>通过action去学习动力学模型.(<em>learn a dynamics model of the environment given an action.</em>)</p>
<p>例如GPS(Guided Policy Search):</p>
<blockquote>
<p>learn the mapping from current state to next best action in a policy search framework</p>
</blockquote>
<h5 id="priors-on-the-observations">Priors on the Observations</h5>
<blockquote>
<p>Regularities can also be encoded in the observation model that <strong>relates the state of the system to the raw sensory signals</strong>.</p>
<p>In the case, where the mapping between state and observation is hand-designed, the state usually refers to some physical quantity.</p>
<p>In the case where the state representation is learned,it is not so easily interpretable.</p>
</blockquote>
<p>实际上就是怎么从 raw sensory signal提取出state。</p>
<p>a)Given/Specified/Engineered Observation Models</p>
<p>基于专家先验手工设计的特征,可以理解为model-based,例如有个物体的数据库模型</p>
<blockquote>
<p>One example are models of multi-view or perspective geometry for camera sensors . Often, approaches also assume access to an object database (OD) that allows them to predict how the objects will be observed through a given sensor.</p>
</blockquote>
<p>b)Learned State Representations</p>
<p>相当于CNN直接输入raw image提取特征。(<em>learn a suitable, task-specific state representation<br>
directly from observations</em>)</p>
<p>C. Does the approach perform action selection?</p>
<p>实际上就是等同于RL中的explore 与exploit的关系</p>
<blockquote>
<p>balance between <strong>exploration (performing an action to improve perception as much as possible)</strong> and <strong>exploitation (performing an action thatmaximizes progress towards the manipulation goal)</strong>.</p>
</blockquote>
<p>D,E,F如字面意思理解, 不做赘述.</p>
<p>D. What is the objective: Perception, Manipulation or Both?</p>
<p>E. Are multiple sensor modalities exploited?</p>
<p>F. How is uncertainty modeled and used?</p>
<h1 id="思考与挑战">思考与挑战</h1>
<p>目前基本都是利用基于视觉的传感器信息,怎么利用更多类型的感知信息?如何选择特征?explore or exploit?</p>

            </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://scott-su.github.io/post/RL-montecarlo-methods">
                  <h3 class="post-title">
                    强化学习之蒙特卡洛方法（Monte Carlo Methods）
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad()
  </script>




  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: '46f718b4bf03f1bd63ba',
        clientSecret: 'fa995d33f696410f0bbee7c2250fefa6e54f5f2d',
        repo: 'Scott-Su.github.io',
        owner: 'Scott-Su',
        admin: ['Scott-Su'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
